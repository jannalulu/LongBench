{"pred": " The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": " The GhostVLAD approach is an extension of the NetVLAD approach, which is used to aggregate local descriptors into global features for place recognition and face recognition. It adds ghost clusters to the NetVLAD clusters to absorb most of the weight during feature aggregation, resulting in better feature discrimination capabilities.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": " Their model outperforms the state of the art results by 68.8% to 71.8% when applied to the IEMOCAP dataset.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": " Context tweets are proposed as an additional feature for neural network models, and a self-matching attention mechanism is applied to improve RNN models.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": " They looked at different pages (and therefore domains and stances) to create a balanced and varied dataset for their experiments.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " Yes, the hashtag and SemEval datasets contain only English data.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": " The proposed task is concept-map-based MDS, the summarization of a document cluster in the form of a concept map.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " The datasets used for evaluation are the CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum.\n\nQuestion: What is the difference between extractive and abstractive summarization?\n\nAnswer: Extractive summarization involves selecting sentences from the source document, while abstractive summarization involves generating new sentences that convey the gist of the document.\n\nQuestion: What is the role of the encoder in the proposed model?\n\nAnswer: The encoder in the proposed model is a pretrained BertSum, which is used to encode the document and obtain representations for its sentences.\n\nQuestion: What is the role of the", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": " The proposed approach, GMKL, outperforms other WSD approaches employing word embeddings on the benchmark word similarity and entailment datasets.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": " The ensemble method works by starting with the best performing model according to validation performance and then adding the best performing model that has not been previously tried, keeping it in the ensemble if it improves its validation performance and discarding it otherwise. This process is repeated until 10 models are selected for the final ensemble.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": " The datasets are Friends and EmotionPush, which are composed of two subsets, Friends and EmotionPush, respectively.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": " The paper focuses on text simplification using neural machine translation (NMT) in the context of simplifying sentences from the English Wikipedia.", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": " The IMDb dataset of movie reviews is used for sentiment analysis.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " The proposed system outperforms strong baseline systems by +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": " Yes", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": " The dataset used for training the Intent Classifier is the set of 124 questions that the users asked, and the training set is increased with 37 classes of intents and 415 samples, with samples per class ranging from 3 to 37.\n\nQuestion: What is the main goal of the test framework?\n\nAnswer: The main goal of the test framework is to provide a tool for integration tests, validate CognIA's implementation, and support the system developers in understanding the behavior of the system and which aspects can be improved.\n\nQuestion: What is the maximum time a simulated user should wait for a bot correct response", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": " The HealthCare and Energy sector achieved the best performance.\n\nQuestion: What is the main contribution of the article?\n\nAnswer: The main contribution of the article is the study of the joint effect of stock news and prices on the daily volatility forecasting problem, using a comprehensive dataset of news headlines at the individual stock level.\n\nQuestion: What is the main limitation of the article?\n\nAnswer: The main limitation of the article is that it relies on proxies for the volatility estimation, which may not be accurate.\n\nQuestion: What is the proposed solution to improve the performance of the GARCH(1,1) model?\n\nAnswer: The proposed solution is to use", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": " RNN-based NMT and Transformer-based NMT models were compared with SMT models on the built dataset.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": " The three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": " The baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, two mature deep learning models on text classification, CNN, and RCNN, SVM and deep learning models with comment information, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments.", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": " The neural network architecture that uses multitask learning achieved the best performance, improving the state-of-the-art performance by several points.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": " Their model improves interpretability by allowing different heads to learn different sparsity behaviors, which can lead to more crisp examples of attention head behavior and novel behaviors unraveled thanks to the sparsity and adaptivity of their proposed model.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models BIBREF15.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": " The XNLI test accuracy and Labeled Attachment Scores (LAS) for zero-shot dependency parsing are used for evaluation.", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": " The attention module is pretrained on the MT encoder.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": " Stylistic patterns are obtained.\n\nQuestion: What is the difference between the two types of classifiers?\n\nAnswer: The two types of classifiers are regular classifiers and MILR classifier.\n\nQuestion: What is the difference between the two types of classifiers?\n\nAnswer: The two types of classifiers are regular classifiers and MILR classifier.\n\nQuestion: What is the difference between the two types of classifiers?\n\nAnswer: The two types of classifiers are regular classifiers and MILR classifier.\n\nQuestion: What is the difference between the two types of classifiers?\n\nAnswer: The two types of classifiers are regular classifiers and MILR classifier.\n\nQuestion: What is the difference between the two", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The encoder has an LSTM (Long Short-Term Memory) layer.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " Yes", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": " The article does not provide specific baseline models or algorithms.", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": " 22,880 users", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": " We use the neural scoring model from BIBREF33 to measure recipe-level coherence for each generated recipe.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": " They create a QA system to comprehend spoken conversations between nurses and patients to extract clinical information.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": " The task-specific encoder is trained on the full training set, which comprises 4,741 medical article abstracts with crowdsourced annotations indicating snippets (sequences) that describe the Participants (p), Interventions (i), and Outcome (o) elements of the respective RCT, and the test set is composed of 191 abstracts with p, i, o sequence annotations from three medical experts.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": " We apply our adaptively sparse Transformers on four machine translation tasks. For comparison, a natural baseline is the standard Transformer architecture using the softmax transform in its multi-head attention mechanisms. We consider two other model variants in our experiments that make use of different normalizing transformations:\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the introduction of sparse attention into the Transformer architecture, showing that it eases interpretability and leads to slight accuracy gains. The paper also proposes an adaptive version of sparse attention, where the shape of each attention head is learnable and can vary continuously and dynamically between the dense limit", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": " The improvement in performance for Estonian in the NER task is significant, as ELMo embeddings show the largest improvement over fastText embeddings on the Estonian dataset.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": " They have a background in the humanities and the social sciences, focusing on the challenges of textual analysis, especially when it involves social and cultural concepts.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " Yes, the paper introduces an unsupervised approach to spam detection using Latent Dirichlet Allocation (LDA) model and topic-based features.", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": " The Nguni languages are similar to each other and harder to distinguish.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": " They compared 6-layers and 9-layers sMBR models with 6-layers and 9-layers Xavier initialization models.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": " The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”). A description of the criteria associated with the different classes can be found in the Wikipedia grading scheme page. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. We constructed the dataset by first crawling all", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a 5-point scale of (Flawless, Good, Non-native, Disfluent, Incomprehensive). For the comparison process, the RNNMorph and the RNNSearch + Word2Vec models’ sentence level translations were individually ranked between each other, permitting the two translations to have ties in the ranking.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": " Models are evaluated based on the efficiency of a communication scheme, which is measured by the fraction of tokens that are kept in the keywords, and the accuracy of the scheme, which is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": " The evaluation metrics for classification tasks are precision, recall, and F-measure.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": " The source domain is the domain with labeled data ( INLINEFORM0 ) and the target domain is the domain with unlabeled data ( INLINEFORM1 ).\n\nQuestion: What is the main challenge of domain adaptation?\n\nAnswer: The main challenge of domain adaptation is that data in the source and target domains are drawn from different distributions, leading to a decline in adaptation performance with an increase in distribution difference.\n\nQuestion: What are the two key limitations of the existing feature adaptation methods for domain adaptation?\n\nAnswer: The two key limitations are that they highly depend on the heuristic selection of pivot features, which may be sensitive to different applications, and they only utilize the", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": " they compare with state-of-the-art methods.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": " NeuronBlocks includes common layers like RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, etc., as well as attention layers like Linear/Bi-linear Attention, Full Attention, Bidirectional attention flow, etc., and regularization layers like Dropout, Layer Norm, Batch Norm, etc.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": " The multilingual pronunciation corpus collected by deri2016grapheme for all experiments.", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": " The baselines were the results obtained by Khandelwal and Sawant (BIBREF12) and the results obtained by the models trained on multiple datasets.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": " They use English, Spanish, and Finnish in their experiment.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": " They test their method on predicting hashtags for a held-out test set of posts, as well as on a larger dataset with an increasing number of hashtags to be predicted.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": " Yes", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": " They generate maps that reflect demographic, linguistic, and psycholinguistic properties of the population represented in the dataset.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": " The ML methods aim to identify argument components in the discourse, which include claims, premises, backing, rebuttal, and refutation.\n\nQuestion: What is the main focus of the experiments conducted on the annotated corpora introduced in the article?\n\nAnswer: The main focus of the experiments is on identifying argument components in the discourse, which includes claims, premises, backing, rebuttal, and refutation.\n\nQuestion: What is the role of the SVMhmm model in the experiments conducted on the annotated corpora?\n\nAnswer: The SVMhmm model is used in the experiments to identify argument components in the discourse, and it is trained on the", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": " PARENT aligns n-grams of order 1.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": " The Twitter dataset consists of 1,873 conversation threads, roughly 14k tweets, and 1.5 million comments.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": " The 12 languages covered in the article are: English, French, Spanish, Mandarin Chinese, Russian, Italian, German, Polish, Finnish, Estonian, Welsh, and Yue Chinese.\n\nQuestion: What is the main goal of the Multi-SimLex initiative?\n\nAnswer: The main goal of the Multi-SimLex initiative is to create a comprehensive and representative lexical semantic similarity dataset for the English language spanning a total of 1,888 concept pairs balanced with respect to similarity, frequency, and concreteness, and covering four word classes: nouns, verbs, adjectives, and adverbs. This dataset serves as the", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": " The two datasets model is applied to are the `Conversations Gone Awry' dataset from BIBREF9 and the subreddit ChangeMyView (CMV) dataset.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": " No, the pipeline components were not based on deep learning models.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": " The quality of the data is empirically evaluated through various sanity checks, including sentence-level BLEU scores, perplexity, and LASER cross-lingual sentence embeddings.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": " They use two RNNs to encode data from the audio signal and textual inputs independently, and then combine the information from these sources using a feed-forward neural model to predict the emotion class.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": " 2.11 BLEU, 1.7 FKGL and 1.07 SARI", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": " unanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": " Tweets went viral if they were retweeted more than 1000 times.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": " LSTM-CRF", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": " The data collection project was mainly supported by Sharif DeepMine company.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": " The machine learning and deep learning methods used for RQE are Logistic Regression and a deep neural network model.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": " The benchmark dataset is the Honeypot dataset, which has been extensively explored in our paper. The quality of the Honeypot dataset is high, as it contains 22,223 spammers and 19,276 legitimate users.", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": " The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, INLINEFORM0 PRO, NOM, SG, 1 INLINEFORM1 .", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " No, the article reports results on English data, but also mentions that the proposed approach is applicable to other languages as well.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": " The best performing model among the author's submissions is the ensemble+ of (r4, r7 r12) on the test set, which was analyzed after test submission. The ensemble+ of (r4, r7 r12) was analyzed after test submission.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": " The baseline was a weak baseline without using any monolingual data, and a strong baseline established with monolingual data.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": " 0.7033", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": " The paper explores embedding techniques such as word embeddings, which are neural network-based approaches that learn a representation of a word by word co-occurrence matrix.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": " They use a pre-ordering system called CFILT-preorder to match words before reordering them.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": " Yes", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": " The experts used for annotation were seven legal experts with legal training.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": " The models used for painting embedding are a sequence-to-sequence model with a single layer unidirectional LSTM encoder and a single layer LSTM decoder, pre-trained retrofitted word embeddings shared between source and target sentences. The models used for language style transfer are a sequence-to-sequence model with a single layer unidirectional LSTM encoder and a single layer LSTM decoder, pre-trained retrofitted word embeddings shared between source and target sentences, and a seq2seq model with global attention.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": " The RNN layer works better than the transformer layer in this context.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": " Yes, the authors hypothesize that humans' robustness to noise is due to their general knowledge.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": " They addressed three different topics of cyberbullying: personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": " They propose extended middle context, a new context representation for CNNs for relation classification.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": " There are three major classes: Person (PER), Location (LOC), and Organization (ORG).", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": " The resulting annotated data is better, as it is improved by the use of both crowd and expert annotations.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": " The data set used to train our ASR system contains 27,085 speech utterances produced by 2,506 speakers, accounting for approximately 100 hours of speech. The evaluation set contains 74,064 speech utterances produced by 1,268 speakers for a total of 70 hours of speech.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": " This approach achieves state of the art results on the Multi30K dataset.", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": " The strong baselines model is compared to the models in BIBREF4, BIBREF6, BIBREF7, BIBREF9, BIBREF11, BIBREF17, BIBREF18, BIBREF20, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF33, BIBREF34, BIBREF35, BIBREF36, BIBREF37, BIBREF38, BIBREF40, B", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": " Logistic Regression (LR) and Multilayer Perceptron (MLP) are used as the target models.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": " They use NLP toolkits like NLTK, Stanford CoreNLP, TwitterNLP, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, and BIBREF26, BIBREF18 for entity-level sentiment analysis and NLP toolkits like BIBREF17, BIBREF19, BIBREF20, BIBREF21, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, and B", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": " Experiments are performed on the SQuAD dataset BIBREF3.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": " The existing approaches have focused on analyzing Flickr tags to extract useful information in domains such as linguistics, geography, and ecology.", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": " Yes, they use attention in the memory generation layer.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": " They used CSAT dataset for CSAT prediction, 20 newsgroups for topic identification task, and Fisher Phase 1 corpus for topic identification task.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": " IMDb movie review dataset", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": " Yes, these tasks were evaluated in previous work, specifically in BIBREF1, BIBREF2, and BIBREF3.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": " Partial support of this work by the Hariri Institute for Computing and Computational Science & Engineering at Boston University (to L.G.) and a Google Faculty Research Award (to M.B. and L.G.) is gratefully acknowledged. Additionally, we would like to thank Daniel Khashabi for his help in running the CogComp-NLP Python API and Mike Thelwal for his help with TensiStrength. We are also grateful to the Stanford NLP group for clarifying some of the questions we had with regards to the Stanford NER tool.", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": " The invertibility condition for our neural projector is that INLINEFORM0 and INLINEFORM1 exist, and INLINEFORM3 is invertible.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": " The proposed qualitative annotation schema for MRC gold standards consists of dimensions of interest such as linguistic complexity, required reasoning, factual correctness, knowledge, and linguistic features. It also includes a taxonomy of the framework and annotation results.", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": " The size of the WikiSmall dataset is 89,042 sentence pairs, and the size of the WikiLarge dataset is 296,402 sentence pairs.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": " The baselines include vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training, and Triangle+pre-train.", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": " The paper studies Natural Language Processing (NLP) and Machine Learning in general, specifically focusing on imbalanced classification problems and the ability to generalise on dissimilar data.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": " The models used in the experiment are a linear SVM trained on word unigrams, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": " unanswerable", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": " Edinburgh embeddings BIBREF14 are used in the system.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": " Our personalized models outperformed the baseline in BPE perplexity, with the Prior Name model achieving the best results. They also generated more diverse and acceptable recipes, and had higher BLEU-1 scores.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": " The combination of rewards for reinforcement learning includes a pre-training process with reinforcement learning and a combination of rewards for irony accuracy, sentiment preservation, and content preservation.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": " The authors demonstrate that their model may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": " They compared to the following systems: BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF29, B", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": " The distribution of each of the identified attributes for the two sets of tweets (those labelled as containing fake news and those labelled as not containing them) is shown in the table and figures provided in the article.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset, which includes 1,108 unique English hashtags from 1,268 randomly selected tweets in the dataset.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": " unanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": " Word subspace can represent sets of word vectors, retaining most of the variability of features and encoding proximity between sets with related words.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": " The baseline model used is Random Forests (RF) BIBREF23 .", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": " No, SemCor3.0 is not reflective of English language data in general. It is a specific corpus of English all-words WSD datasets from the Senseval and SemEval competitions, which includes five standard all-words fine-grained WSD datasets from the Senseval and SemEval competitions.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": " The Augmented LibriSpeech dataset is 100 hours long.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": " The dataset for fine-grained classification is split in training, development, development_test and test parts.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": " BERT is used in this paper, and the number of Transformer blocks is 12, the number of the hidden layer is 768, the number of self-attention heads is 12, and the total number of parameters of the pre-trained model is 110M.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": " Yes, the automatically constructed datasets are subject to quality control, as evidenced by the careful manipulation and control of the complexity of target questions, as well as the use of systematic biases in the initial construction of the datasets.", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": " unanswerable", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": " Their performance on emotion detection was competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": " The tagging scheme employed is the INLINEFORM0 scheme, which consists of three tags: INLINEFORM0, INLINEFORM1, and INLINEFORM2.", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": " No, Arabic is not one of the 11 languages in CoVost.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": " The model is considered robust if it is able to handle situations where prior knowledge is biased or insufficient, such as when there are unbalanced labeled features or class distributions.", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": " Average BERT embeddings or using the CLS-token output from a BERT network achieved bad results for various STS tasks (Table TABREF6), worse than average GloVe embeddings. However, for SentEval, average BERT embeddings or the BERT CLS-token output achieves decent results (Table TABREF15), outperforming average GloVe embeddings.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": " The proposed DSC loss outperforms BERT-MRC by +0.29 and +0.96 respectively on English datasets, and achieves F1 improvements by +0.97 and +2.36 on Chinese datasets.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": " They test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": " They compared against syntactic tree-based models, latent tree-based models, and non-tree models.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": " The core component for KBQA is the improved relation detection model, HR-BiLSTM, which performs hierarchical matching between questions and KB relations.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": " The baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec).", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": " The methods considered to find examples of biases and unwarranted inferences are: a browser-based annotation tool, tagging descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": " The article explores the use of gender distinctions in languages, specifically in the Romance languages (French, Spanish, Italian, Portuguese) and Semitic languages (Arabic, Hebrew).", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": " They experimented with models that use plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrate lower contexts via peephole connections.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": " No, the article reports results on English data.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": " The authors experimented with Sumy algorithms, which require a similar parameter K to determine the number of sentences to keep in the final summary.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": " The previous state of the art for this task was proposed by BIBREF0.", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": " The number of MP iterations.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": " The task was based on an annotated data set to guarantee objective reasoning throughout different approaches. The corpus used in the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19, consisting of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": " They experiment with 7 Indian languages: Kannada, Hindi, Telugu, Malayalam, Bengali, and English.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": " The model performance on target language reading comprehension is shown in Table TABREF6, where the results of different models trained on either Chinese or English and tested on Chinese are presented. The top half of the table shows the results of training data without translation, and the lower half shows the results of translated training data. The results show that the performance of multi-BERT fine-tuned on different languages and then tested on English, Chinese, and Korean is relatively high, with the top half of the table showing the results of training data without translation and the lower half showing the results of translated training data.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": " ALOHA, combined with the HLAs and dialogue dataset, achieves a significant boost on the target character language style retrieval task compared to the baseline open-domain chatbot models.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": " ARAML improves the performance of text generation tasks with no explicit evaluation metrics as rewards, outperforming other GAN baselines in terms of both forward and reverse perplexity, and achieving a smaller standard deviation in training stability.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": " The authors present evidence that the model can capture some biases in data annotation and collection by examining the test datasets and their confusion matrices. They find that the majority of errors are related to hate class where the model misclassified hate content as offensive in 63% of the cases. They also mention that even for a human it is difficult to discriminate against this kind of implicit abuses.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": " Yes", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": " The dataset is in standard CoNLL-2003 IO formatBIBREF25.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": " The proposed DSC loss performs robustly on all the three datasets.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": " The datasets used in this work include EEG data from BIBREF0, self-paced reading time data, and ERP data from BIBREF0.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": " The subjects were presented with a series of stimuli, including words, pictures, and sounds, to elicit event-related responses.", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, and Pointer-Gen+ARL-ROUGE are used for evaluation.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": " Traditional machine learning classifiers and neural network based models are used on the dataset.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": " Bi-directional and uni-directional language models are used.", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": " The weights of training examples are dynamically adjusted as training proceeds, changing as training proceeds.", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": " The results show that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": " An individual model consists of a Bayesian model for each language and crosslingual latent variables to incorporate soft role agreement between aligned constituents.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": " The non-standard pronunciation is identified by the presence of foreign words, in this case Spanish words, in the Mapudungun speech.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": " A semicharacter architecture is a type of semi-character based RNN (Recurrent Neural Network) that processes a sentence of words with misspelled characters, predicting the correct words at each step. It treats the first and the last characters individually, and is agnostic to the ordering of the internal characters.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": " The article explores 16 languages, including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": " NCEL outperforms the state-of-the-art collective methods across five different datasets, demonstrating good generalization ability.", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": " The baseline used was the performance of the error detection system by Rei2016, trained using the same FCE dataset.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": " The annotated clinical notes were obtained from the CE task in 2010 i2b2/VA.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": " Masking words in the decoder is helpful because it allows the decoder to focus on one word at a time, based on the source document as well as other words, which helps in generating more fluent and natural sequences.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": " They use the Paraphrase Database (PPDB) for building representation models based on supervision from richly structured resources.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": " TF-IDF features are used to transform a pathology report into a feature vector.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": " The dataset is annotated based on a hierarchical model of depression-related symptoms, with each tweet being annotated as no evidence of depression or evidence of depression, and further annotated with one or more depressive symptoms.", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": " They evaluated on eight biomedical NER tasks.", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": " The training data was translated into Spanish, using the machine translation platform Apertium.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": " They used a content-based classifier in conjunction with two feature selection methods.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": " The baseline for the SLC task is a very simple logistic regression classifier with default parameters, where they represent the input instances with a single feature: the length of the sentence.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": " They compare with a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger, n-grams, label transitions, word suffixes, and relative position to the end of the text are considered.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": " The political bias of different news sources is included in the model by assigning a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": " The ancient Chinese dataset comes from the internet, specifically from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": " English", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": " The compound PCFG and neural PCFG were trained on the Chinese dataset from BIBREF56.", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": " The UTCNN model has three convolutional layers and an average pooling layer.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": " The dataset used in this paper is the 70 million Flickr photos with coordinates in Europe, all of which were uploaded to Flickr before the end of September 2015.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": " The clinical datasets used in the paper are NUBes-PHI and MEDDOCAN.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": " They used unigrams and pragmatic features, stylistic patterns, Hastag interpretations, and textual features.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are the predictive quality and strategy formulation ability.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": " Yes, they employ their indexing-based method to create a sample of a QA Wikipedia dataset.", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": " The targets are Galatasaray (Target-1) and Fenerbahçe (Target-2), two popular football clubs in Turkey.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": " Additional experiments are conducted on the transformation from ironic sentences to non-ironic sentences, showing that our model still can achieve the second-best results in sentiment and content preservation, but DualRL and ours get poor performances in irony accuracy.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": " Gaussian-masked directional multi-head attention is a variant of multi-head self-attention that uses a Gaussian weight matrix to capture localness and directional information for the importance of adjacent characters. It combines the Gaussian weight matrix with the score matrix produced by the self-attention to generate the representation of input.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": " They considered social media texts, specifically tweets.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": " The network's baseline features are extracted from the pre-trained models and used to classify a sentence as sarcastic or non-sarcastic.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": " The number of clusters (k) and the dimensionality of the word embeddings (d) were varied in the experiments on the four tasks.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": " Their official scores (column Ens Test in Table TABREF19 ) have placed them second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": " The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": " Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": " They consider text categorization, sentiment classification, and text classification tasks.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": " Their model is compared to rule-based methods, hand-crafted or semi-automated syntactic or semantic extraction rules, and learned methods.", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": " The size of the training sets of these versions of ELMo compared to the previous ones is significantly larger, as they were trained on larger monolingual corpora from various sources for each language.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": " The dataset contains 6946 sentences.", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": " They compare to MLP, Eusboost, and MWMOTE.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": " No, they evaluate on both English and Chinese datasets.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": " Highest 'MRR' score for Factoid Question Answering task in the third test batch set.", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": " They evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank, which is a subset of the Penn Treebank corpus.", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": " The authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by stating that \"building models under these frameworks requires a large overhead of mastering these framework details.\"", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": " They achieve the state of the art on WebQSP and SimpleQuestions.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
